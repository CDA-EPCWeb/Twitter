{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importa√ß√µes necess√°rias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from geobr import read_municipality\n",
    "import string\n",
    "import re\n",
    "import unidecode\n",
    "import numpy as np\n",
    "from IPython.display import clear_output \n",
    "from os import system, name \n",
    "from time import sleep \n",
    "\n",
    "###Fun√ß√£o que busca uma determinada palavra em uma string (se n√£o retorna \"None\" √© porque a palavra existe) \n",
    "def achar_palavra(palavra):\n",
    "    return re.compile(r'\\b({0})\\b'.format(palavra), flags=re.IGNORECASE).search\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aten√ß√£o\n",
    "    Defina na c√©lula a seguir em quais semanas ser√° realizada a corre√ß√£o das localiza√ß√µes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Iniciando corre√ß√£o...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recuperando informa√ß√µes oficiais sobre as cidades do Brasil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataframe com os munic√≠pios oficiais do Brasil e respectivos estados\n",
    "geobr_mun = read_municipality(code_muni=\"all\", year=2019)[[\"code_muni\",\"name_muni\",\"abbrev_state\"]]\n",
    "\n",
    "#corrige o formato da coluna code_muni, para que os c√≥digos dos munic√≠pios sejam inteiros\n",
    "geobr_mun = geobr_mun.astype({'code_muni':int})\n",
    "\n",
    "#Transforma o c√≥d do munic√≠pio no index do dataframe\n",
    "geobr_mun.set_index('code_muni', inplace=True)\n",
    "\n",
    "#geobr_mun.tail(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fun√ß√£o util adiante para tratar abrevia√ß√µes em nomes de cidades. Ex: \"Machadinho D'oeste\" -> \"Machadinho do Oeste\"\n",
    "def desabrevia_dos(texto):\n",
    "    for abrev in [\"D'\", \"d'\"]:\n",
    "        if abrev in texto:\n",
    "            texto = texto.replace(abrev,\"do \")\n",
    "    return texto\n",
    "\n",
    "\n",
    "def adequa_base(cidade):\n",
    "    nome_desabreviado = desabrevia_dos(cidade).lower() #desabrevia e deixa em min√∫sculo\n",
    "    nome_sem_acentos = unidecode.unidecode(nome_desabreviado) #retirar acentos\n",
    "    return nome_sem_acentos\n",
    "\n",
    "\n",
    "serie_cidades_brasil = geobr_mun['name_muni']\n",
    "for i, item in serie_cidades_brasil.iteritems():\n",
    "    serie_cidades_brasil.loc[i] = adequa_base(item)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recuperando as localidades coletadas do Twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataframe com os munic√≠pios coletados do twitter em cada semana\n",
    "#path = os.getcwd() + '\\\\results\\\\week_'\n",
    "#filename = 'tweets_locations_' \n",
    "path = os.getcwd() + '/results/week_'\n",
    "filename = 'tweets_locations_'\n",
    "\n",
    "df_locations = pd.DataFrame(None)\n",
    "\n",
    "#l√™ os arquivos das semanas definidas no in√≠cio\n",
    "for i in range(1,38):#(1,38)\n",
    "    clear_output()\n",
    "    print(f'Acessando semana {i}')\n",
    "    tempDF = pd.read_csv(path + str(i) + '\\\\' + filename + str(i) + '.csv', sep=';') #<-no linux, comente essa linha e descomente a de baixo\n",
    "    #tempDF = pd.read_csv(path + str(i) + '/' + filename + str(i) + '.csv', sep=';') \n",
    "    tempDF = tempDF.assign(week=i)\n",
    "    df_locations = pd.concat([df_locations, tempDF])\n",
    "\n",
    "\n",
    "#Armaneza em outro df e exclui do original linhas com entradas nulas em alguma das colunas ('location' ou 'count')    \n",
    "df_entradas_nulas = df_locations[df_locations.isnull().any(axis=1)]\n",
    "df_entradas_nulas.reset_index(drop=True,inplace=True)\n",
    "df_locations.dropna(inplace=True)  \n",
    "\n",
    "\n",
    "#Armaneza em outro df e exclui do original linhas com entradas que cont√©m apenas espa√ßos em branco na coluna 'location'\n",
    "df_entradas_em_branco = df_locations[df_locations['location'].apply(lambda x: True if str.isspace(x) else False)]\n",
    "df_entradas_em_branco.reset_index(drop=True,inplace=True)\n",
    "df_locations = df_locations[df_locations['location'].apply(lambda x: False if str.isspace(x) else True)]\n",
    "\n",
    "\n",
    "#conserta os √≠ndices\n",
    "df_locations.reset_index(drop=True,inplace=True)\n",
    "\n",
    "#DESCRI√á√ÉO: Contagem de quantos tweets com determinada localiza√ß√£o apareceram em determinada semana\n",
    "#Obs: se o mesmo usu√°rio fez mais de um tweets, a localiza√ß√£o dele aparecer√° mais de uma vez\n",
    "#df_locations.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corrigindo os textos dos tweets coletados "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fun√ß√£o para tratamento do texto contido no campo \"location\" nos tweets coletados\n",
    "#Fonte: https://www.analyticsvidhya.com/blog/2020/11/text-cleaning-nltk-library/\n",
    "#fonte: https://www.geeksforgeeks.org/python-efficient-text-data-cleaning/\n",
    "\n",
    "def trata_texto(texto_entrada):\n",
    "    \n",
    "    #Varias localidades vem no formato \"cidade, estado\" e estamos interessados s√≥ na cidade\n",
    "    texto_entrada = texto_entrada.split(\",\")[0]\n",
    "        \n",
    "    #\n",
    "    texto_sem_hiperlinks = re.sub(r'https?:\\/\\/.\\S+', \"\", texto_entrada) #pode deixar espa√ßos extra\n",
    "    \n",
    "    #\n",
    "    texto_sem_mais_e_hifen = re.sub(\"\\+\",\" \",re.sub(\"\\-\",\" \", texto_sem_hiperlinks)) #prejudica hiperlinks\n",
    "    \n",
    "    #\n",
    "    texto_sem_hash = re.sub(r'#', '', texto_sem_mais_e_hifen)\n",
    "    \n",
    "    #verifica se \"do\" est√° abreviado no texto e corrige .Ex: \"Machadinho D'oeste\" -> \"Machadinho do oeste\"\n",
    "    texto_desabreviado = desabrevia_dos(texto_sem_hash)\n",
    "    \n",
    "    #    \n",
    "    texto_sem_outras_pontuacoes = \"\".join([i for i in texto_desabreviado if i not in string.punctuation])\n",
    "    \n",
    "    #Aten√ß√£o -> s√≥ consegue separar palavras onde as duas comecem com mai√∫scula, como em S√£oPaulo, mas n√£o em S√£opaulo\n",
    "    texto_sem_palavras_juntas = \" \".join([s for s in re.split(\"([A-Z][a-z]+[^A-Z]*)\",texto_sem_outras_pontuacoes) if s])  \n",
    "    \n",
    "    #\n",
    "    texto_sem_espa√ßos_extra = re.sub(\"\\s+\",\" \", texto_sem_palavras_juntas)\n",
    "    \n",
    "    #\n",
    "    texto_em_minusculo = texto_sem_espa√ßos_extra.lower()\n",
    "    \n",
    "    #\n",
    "    texto_sem_acentos = unidecode.unidecode(texto_em_minusculo)\n",
    "    \n",
    "    \n",
    "    return texto_sem_acentos\n",
    "\n",
    "#texto de teste\n",
    "#trata_texto (\"MaisNot√≠cias D'oeste: voc√™, pode ---- encontrar,, mais #informa√ß√µes em https://www.analyticsvidhya.com/blog/2020/11/text-cleaning-nltk-library/ ou em outros lugares como em S√£oPaulo+SP. üçÄüåª\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corrigindo a localiza√ß√µes dos tweets\n",
    "    O processo ser√° realizado a partir dos seguintes dados:\n",
    "    - geobr_mun ---> dataframe que cont√©m os dados oficiais dos munic√≠pios do Brasil\n",
    "    - dict_cidades ----> dicion√°rio com as mesmas cidades de geobr_mun, mas em um formato facilmente identific√°vel\n",
    "    - df_locations_tratado ----> dataframe com as localiza√ß√µes dos tweets j√° limpo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#criar fun√ß√£o que recebe o valor de uma celula da coluna location e tenta localizar ocorr√™ncias\n",
    "\n",
    "#Essa fun√ß√£o √© demorada\n",
    "def corrigir_cidades(localizacao_do_tweet):\n",
    "    #----------------------Identifica poss√≠veis locais e guarda em um dicion√°rio\n",
    "    locais_identificados = {}\n",
    "    for key, item in serie_cidades_brasil.items(): #<--------------garg√°lo aqui!\n",
    "        if achar_palavra(item)(localizacao_do_tweet) != None:\n",
    "            locais_identificados[item] = key #locais_identificados = {cidade:c√≥d}   \n",
    "    \n",
    "    #----------------------Seleciona o local com nome maior \n",
    "    if locais_identificados != {}:\n",
    "        maior_nome_cidade = sorted(list(locais_identificados.keys()), key=len)[-1]\n",
    "        index_do_maior = locais_identificados[maior_nome_cidade]\n",
    "        \n",
    "        dicio = geobr_mun.loc[[index_do_maior]].to_dict('records')[0]\n",
    "        dicio['code_muni'] = index_do_maior         \n",
    "        return dicio\n",
    "    else:\n",
    "        return ('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fun√ß√£o para limpar o output (n√£o afeta o resultado)\n",
    "# define our clear function \n",
    "def clear(): \n",
    "  \n",
    "    # for windows \n",
    "    if name == 'nt': \n",
    "        _ = system('cls') \n",
    "        \n",
    "    # for mac and linux(here, os.name is 'posix') \n",
    "    else: \n",
    "        _ = system('clear') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aten√ß√£o!\n",
    "    A c√©lula abaixo realiza a identifica√ß√£o das cidades, armazenando o resultado no dataframe df_locations_tratado\n",
    "    O PROCESSO PODE LEVAR ALGUMAS (OU MUITAS) HORAS!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_locations = df_locations.sample(10) sample para teste\n",
    "\n",
    "tam_dataframe = len(df_locations)\n",
    "contador = 1\n",
    "\n",
    "for i, row in df_locations.iterrows():\n",
    "    clear()\n",
    "    clear_output()\n",
    "    print(f'{contador} de {tam_dataframe} linhas analisadas ({(contador/tam_dataframe)*100}%)')\n",
    "    \n",
    "    dict_cidade_corrigida = corrigir_cidades(trata_texto(row['location'])) #<<<---garg√°lo na fun√ß√£o corrigir_cidades\n",
    "                                                          \n",
    "    if (dict_cidade_corrigida!=''):\n",
    "        \n",
    "        print(dict_cidade_corrigida['name_muni'])\n",
    "        \n",
    "        df_locations.loc[[i], ['name_muni']] = dict_cidade_corrigida['name_muni']\n",
    "        df_locations.loc[[i], ['abbrev_state']] = dict_cidade_corrigida['abbrev_state']\n",
    "        df_locations.loc[[i], ['code_muni']] = dict_cidade_corrigida['code_muni']\n",
    "\n",
    "    #if (contador==15):\n",
    "    #    break;\n",
    "    \n",
    "    contador+=1\n",
    "\n",
    "df_locations.reset_index(inplace=True, drop=True)\n",
    "df_locations.to_csv(f'tweets_location_corrigido.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
